I"`<h1 id="generalized-linear-model">Generalized Linear Model</h1>

<blockquote>
  <p>Since there are some problems about mathjax, I also upload the pdf version <a href="https://denglinsui.github.io/pdf/StatisticsModel/01.pdf" target="_blank">„ÄåStatistics Model„Äç1 Generalized Linear Model</a>.</p>

</blockquote>

<h2 id="why-do-we-need-glm">Why do we need GLM?</h2>

<p>Ordinary linear regression predicts the expected value of a given unknown quantity as a linear combination of a set of observed values. This is appropriate when the response variable has a normal distribution.  However, these assumptions are inappropriate for some types of response variables.</p>

<p>For example, when the response is not normal, e.g. binary data, the linear model is inappropriate. Sometimes, the output doesn‚Äôt linearly change with respect to input. Therefore, the ordinary linear regression need to be extended.</p>

<h2 id="definition-of-glm">Definition of GLM</h2>

<p>Both of OLM and GLM use the <strong>linear predictor</strong> $\eta=x^\top \beta$  through $\mu=\mathbb{E}(y)$.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">OLM</th>
      <th style="text-align: center">GLM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$y\sim\mathcal{N}(\mu,\sigma)$</td>
      <td style="text-align: center">$f(y ; \theta, \phi)=\exp { \frac{y \theta-b(\theta)}{\phi}+c(y ; \phi)}$</td>
    </tr>
    <tr>
      <td style="text-align: center">$\mu=\eta$</td>
      <td style="text-align: center">$\eta=g(\mu)$</td>
    </tr>
  </tbody>
</table>

<p>where $\theta$  depends on the <strong>linear predictor</strong>, the <strong>dispersion parameter</strong> $\phi$ is
often <em>known</em>, and $g$ is a monotone <strong>link function</strong>.</p>

<p>The GLM generalized OLM in the aspects of the distribution of response variables and the link between $\mu$ and $\eta$ (nonlinear relations between the mean response and the covariates).</p>

<h3 id="exponential-family">Exponential Family</h3>

<p>The GLM extend the distribution of $y$ from the normal distribution to the exponential family. There are some useful properties:</p>

<ul>
  <li>Moment-generating function: $M(t)=\exp {b(\theta+t \phi)-b(\theta)}$</li>
  <li>Mean: $\mathrm{E}(Y)=b^{\prime}(\theta)=\mu$</li>
  <li>Variance: $\operatorname{var}(Y)=\phi b^{\prime \prime}(\theta)=\phi b^{\prime \prime}{b^{\prime-1}(\mu)}=\phi V(\mu)$</li>
</ul>

<p>where $V(\mu)$ is the <strong>variance function</strong>.</p>

<p><u>Note:</u> If $\phi$ is known, the variance function $V(\mu)$ and its domain $\Rightarrow\kappa(\theta)\Rightarrow m(\theta)\Rightarrow$ distribution.</p>

<h3 id="link-function-gmu">Link Function $g(\mu)$</h3>

<p>The link function provides the relationship between the linear predictor and the mean of the distribution function.</p>

<p>The link function $g$ should remove restrictions on $\mu$ by mapping to $\eta$:</p>

\[\eta_i=g(\mu_i)\]

<p><strong>Canonical link</strong>: $\eta=\theta=b‚Äô^{-1}(\mu)$. When $\phi$ is known, the model is a natural exponential family.</p>

<h2 id="estimation-and-inference">Estimation and Inference</h2>

<h3 id="discussion-of-beta">Discussion of $\beta$</h3>

<h4 id="estimation-of-beta">Estimation of $\beta$</h4>

<p>To estimate $\beta$, with Chain rule, we solve the equation:</p>

\[\begin{array}{l}
\frac{\partial \ell(\beta)}{\partial \beta}=\frac{\partial \eta^{\mathrm{T}}}{\partial \beta} \frac{\partial \theta}{\partial \eta^{\mathrm{T}}} \frac{\partial \ell}{\partial \theta^{\mathrm{T}}}=X^{\mathrm{T}} u(\beta)
\end{array}\]

<p>$\beta$ and $\theta$ interacts through $\eta=X^\top\beta$.  That is, $\beta-\eta-\mu-\theta$.</p>

<p>For the exponential familiy, the elements of score statistics $u(\beta)$ and weighted matrix $W(\beta)$  are</p>

\[\begin{array}{l}
u_{j}=\frac{\partial \theta_{j}}{\partial \eta_{j}} \frac{\partial \ell_{j}\left(\theta_{j}\right)}{\partial \theta_{j}}=\frac{y_{j}-\mu_{j}}{g^{\prime}\left(\mu_{j}\right) \phi_{j} V\left(\mu_{j}\right)} \\
w_{j}=\left(\frac{\partial \theta_{j}}{\partial \eta_{j}}\right)^{2} \frac{\partial^{2} \ell_{j}\left(\theta_{j}\right)}{\partial \theta_{j}^{2}}=\frac{1}{g^{\prime}\left(\mu_{j}\right)^{2} \phi_{j} V\left(\mu_{j}\right)}
\end{array}\]

<p>So, given $I(\beta)=X(\beta)^{\mathrm{T}} W(\beta) X(\beta)$, we can find $\beta$ by iterative weighted least squres mothod:</p>

\[\widehat{\beta} \doteq \beta+I(\beta)^{-1} \frac{\partial \eta^{\mathrm{T}}(\beta)}{\partial \beta} u(\beta)\]

<p>In this case,  $y$ is a possible initial value for $\mu$  and $g(y)$ is for $\eta$.</p>

<h4 id="inference-of-beta">Inference of $\beta$</h4>

<p>Under regularity conditions,</p>

\[\widehat{\beta}\dot{\sim}\mathcal{N}(\beta,(X^\top WX)^{-1})\]

<h3 id="discussion-of-phi">Discussion of $\phi$</h3>

<p>At the discussion above, we assume $\phi_j$ are known. But if they are unknown and have $\phi_j=a_j\phi$, $\phi$ need to be estimated.</p>

<h4 id="estimstion-of-phi">Estimstion of $\phi$</h4>

<p>Note that,  $\phi=\operatorname{var}\left(y_{j}\right) /{a_{j} V\left(\mu_{j}\right)}$.</p>

<p>If $\beta$ were known,</p>

\[\widehat{\phi}=\frac{1}{n} \sum_{j=1}^{n} \frac{\left(y_{j}-\mu_{j}\right)^{2}}{a_{j} V\left(\widehat{\mu}_{j}\right)}\]

<p>If $\beta$ were unkown,</p>

\[\widehat{\phi}=\frac{1}{n-p} \sum_{j=1}^{n} \frac{\left(y_{j}-\widehat{\mu}_{j}\right)^{2}}{a_{j} V\left(\widehat{\mu}_{j}\right)}\]

\[P=\frac{1}{\phi} \sum_{j=1}^{n} \frac{\left(y_{j}-\widehat{\mu}_{j}\right)^{2}}{V\left(\widehat{\mu}_{j}\right) / a_{j}}\]

<h3 id="fit-measure">Fit Measure</h3>

<h4 id="analysis-of-deviance">Analysis of Deviance</h4>

<p>Use scaled deviance:
\(D^\star=2\{l_{max}-l(\widehat{\beta})\}\dot{\sim} \chi^2_{n-p}\)</p>

<h4 id="pearsons-statistics">Pearson‚Äôs Statistics</h4>

\[P=\frac{1}{\phi} \sum_{j=1}^{n} \frac{\left(y_{j}-\widehat{\mu}_{j}\right)^{2}}{V\left(\widehat{\mu}_{j}\right)  a_{j}}\dot{\sim}\chi^2_{n-p}\]

<p><u>Note:</u>  Their distributions depend on the situation.</p>

<h4 id="full-model-vs-reduced-model">Full Model v.s. Reduced Model</h4>

<p>For model A and model B:</p>

\[D_a-D_B\dot{\sim} \chi^2_{p_A-p_B}\]

<h1 id="references">References</h1>

<ul>
  <li><a href="https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf">Introduction to Generalized Linear Models</a></li>
  <li><a href="https://www.cambridge.org/core/books/statistical-models/8EC19F80551F52D4C58FAA2022048FC7?__cf_chl_jschl_tk__=aa921ed4560dcaea177e8da320fca59b236ef827-1593743587-0-Aced3me35WQuzFYEtvpkZ_Elir4Gt9CInH2WMwxG_WMgu4KEpsi7sRFlcnKh7V23HK1UMQFiSC5tiTEtuo9sT_C1lnAlJ5k9gVej2S3NqvLdnMPR3JlpJ4tR3sNiaE2m7rCjabSX1l32yLgl6CS83-fxUdQoBmiU6KuWIdy14rAD-SYlV22sSmUD9CxSp5gCS2rnf_ip0AsWuC21P-XuwRh9uZZLDtfqLu4K5kjapJfsT2QB7Beeb2ljamMYfL3vm0t9FUs5S02iNGs89CtSdGA25F3XxEyF9IPVtZlfkvhNFWh-DOxW1JbbsmznYnxyC82lgvqZxQSgnVcwUQXqfWRyzET6iqsyMgG6L19WTHD2H8N74h-Sz18oB3cn-XX9b08yXYm7AKYzvR3NV7eh_f-sL15-pI9aMIZaN-ub2YfW">Statstical Models</a></li>
</ul>
:ET